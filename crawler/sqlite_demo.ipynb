{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "import requests\n",
    "from datetime import datetime\n",
    "# import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url =\"https://edwardzou.blogspot.com/?fbclid=IwAR3HsqRaK0D4sbuDJX7agJwfQc2uLBtc9V0PRJR0uT3u3pGEF0F9AL3JVVw\"\n",
    "url = main_url\n",
    "response = requests.get(url)\n",
    "html = BeautifulSoup(response.text)\n",
    "boxes = html.find(\"div\", id=\"content-wrapper\").find_all(\"div\",class_=\"box-body-deg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【邁向Apple大師之路】如何在Mac使用Windows 系統？Bootcamp VS 虛擬機\n",
      "https://edwardzou.blogspot.com/2019/06/macUseWindows.html\n",
      "【邁向Apple大師之路】Mac 入門及基本設定\n",
      "https://edwardzou.blogspot.com/2019/06/macStart.html\n",
      "【邁向Apple大師之路】iPhone怎麼快速充電?\n",
      "https://edwardzou.blogspot.com/2019/04/iPhoneCharging.html\n",
      "【日本自由行】岐阜縣飛驒高山古街,美麗的江戶時代風格街道\n",
      "https://edwardzou.blogspot.com/2019/04/takayamastreet.html\n",
      "【日本自由行】岐阜縣飛驒高山宮川朝市,日本三大早市之一\n",
      "https://edwardzou.blogspot.com/2019/04/asaichi.html\n",
      "【Angular】如何在token失效並重新登入後，重發前次Request?\n",
      "https://edwardzou.blogspot.com/2019/04/angularResendRequest.html\n",
      "【邁向Apple大師之路】不佔iCloud空間的共享相簿,優點與缺點分析\n",
      "https://edwardzou.blogspot.com/2019/04/sharePhoto.html\n",
      "【開箱文】匿名2 金色手搖磨豆機\n",
      "https://edwardzou.blogspot.com/2019/03/coffeeGrinder.html\n",
      "【食記】台北信義區 PAUL 法式麵包餐廳, 甜點不錯鹹食普普\n",
      "https://edwardzou.blogspot.com/2019/03/paul.html\n",
      "【教學】在iPhone上使用Suica西瓜卡\n",
      "https://edwardzou.blogspot.com/2019/02/suica.html\n",
      "【Angular】ngx-translate 多語系實務應用\n",
      "https://edwardzou.blogspot.com/2019/01/ngx-translate.html\n",
      "【邁向Apple大師之路】在iPhone 上限制App使用時間, iOS 12 的新功能\n",
      "https://edwardzou.blogspot.com/2018/12/app-limit.html\n"
     ]
    }
   ],
   "source": [
    "for box in boxes:\n",
    "    print(box.find(\"a\")[\"title\"])\n",
    "    print(box.find(\"a\")[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def crawler_main(pages, newest_update):\n",
    "    main_url = \"https://nba.udn.com/nba/index?gr=www\"\n",
    "    domain_url = \"https://nba.udn.com\"\n",
    "    # go to main page > 焦點新聞 more\n",
    "    newest_update = datetime.strptime(newest_update, \"%Y-%m-%d %H:%M\")\n",
    "    url = main_url\n",
    "    response = requests.get(url)\n",
    "    html = BeautifulSoup(response.text)\n",
    "    news_page = domain_url + html.find(\"div\", id=\"mainbar\").find(\"div\", id=\"news\").find(\"h1\", class_=\"box-title\").find(\"a\")[\"href\"]\n",
    "    # https://nba.udn.com/nba/cate/6754\n",
    "    # result at 0617\n",
    "    # news_page = \"https://nba.udn.com/nba/cate/6754\"\n",
    "    # go page1_newest\n",
    "    news_articles = []\n",
    "    for c_page in range(1, pages + 1):\n",
    "        url = news_page + \"/-1/newest/\" + str(c_page)\n",
    "        response = requests.get(url)\n",
    "        html = BeautifulSoup(response.text)\n",
    "        body_box = html.find(\"div\", id=\"news_list_body\").find(\"dl\").find_all(\"dt\")\n",
    "        b = 0\n",
    "        for box in body_box:\n",
    "            url = domain_url + box.find(\"a\")[\"href\"]\n",
    "            update_time = get_article_update_time(url)\n",
    "            check_update_time = datetime.strptime(update_time, \"%Y-%m-%d %H:%M\")\n",
    "            title = box.find(\"h3\").text\n",
    "            if check_update_time <= newest_update:\n",
    "                b = 1\n",
    "                break\n",
    "            content = get_article_content(url)\n",
    "            news_articles.append([url, title, update_time, content])\n",
    "        if b:\n",
    "            break\n",
    "    return news_articles\n",
    "\n",
    "\n",
    "def get_article_update_time(article_url):\n",
    "    #     article_url = \"https://nba.udn.com/nba/story/6780/3877002\"\n",
    "    response = requests.get(article_url)\n",
    "    html = BeautifulSoup(response.text)\n",
    "    aritle_update_time = html.find(\"div\", id=\"shareBar\").find(\"div\", class_=\"shareBar__info\") \\\n",
    "        .find(\"div\", class_=\"shareBar__info--author\").find(\"span\").text\n",
    "    return aritle_update_time\n",
    "\n",
    "\n",
    "def get_article_content(article_url):\n",
    "    #     aritle_url = \"https://nba.udn.com/nba/story/6780/3877002\"\n",
    "    result = \"\"\n",
    "    response = requests.get(article_url)\n",
    "    html = BeautifulSoup(response.text)\n",
    "    article_content = html.find(\"div\", id=\"story_body_content\").find_all(\"p\")\n",
    "    for c in article_content:\n",
    "        result = result + c.text\n",
    "    delete_content = \" Getty Imagesfacebooktwitterpinterest\"\n",
    "    result = result.replace(delete_content, \"\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     news = crawler_main(2, \"2019-06-17 09:54\")\n",
    "#     df = pd.DataFrame(news, columns=[\"url\", \"title\", \"update_time\",\"content\"])\n",
    "#     print(df)\n",
    "#\n",
    "#\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 20 data get\n",
      "data_updated\n"
     ]
    }
   ],
   "source": [
    "from crawler_NBA import crawler_main\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 先檢查有最新資料\n",
    "conn = sqlite3.connect(\"../db.sqlite3\")\n",
    "c = conn.cursor()\n",
    "sel = c.execute(\"select update_time from news order by update_time -1 limit 1\")\n",
    "conn.commit()\n",
    "lastest_update = \"\"\n",
    "for data in sel:\n",
    "        lastest_update = data[0]\n",
    "        print(lastest_update)\n",
    "if lastest_update == \"\":\n",
    "    lastest_update = \"2019-06-15 00:00\"\n",
    "conn.close()\n",
    "\n",
    "# 依照最新資料去爬\n",
    "# default 爬最新的五頁 ~ 最新資料為止\n",
    "\n",
    "news = crawler_main(2, lastest_update)\n",
    "insert_data = []\n",
    "print(\"total:\", len(news), \"data get\")\n",
    "for data in news:\n",
    "    url, title, update_time, content = data\n",
    "#     update_time = datetime.strptime(update_time, \"%Y-%m-%d %H:%M\")\n",
    "    created = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "    insert_data.append((url, title, update_time, content,created,))\n",
    "\n",
    "conn = sqlite3.connect(\"../db.sqlite3\")\n",
    "c = conn.cursor()\n",
    "c.executemany('INSERT INTO news VALUES (null,?,?,?,?,?)', insert_data)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"data_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRUNCATE TABLE\n",
    "# from crawler_NBA import crawler_main\n",
    "# import warnings\n",
    "# from datetime import datetime\n",
    "# import sqlite3\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# # 先檢查有最新資料\n",
    "# conn = sqlite3.connect(\"../db_backup.sqlite3\")\n",
    "# c = conn.cursor()\n",
    "# c.execute(\"DELETE FROM news\")\n",
    "# conn.commit()\n",
    "# # for data in sel:\n",
    "# #     lastest = data[0]\n",
    "# conn.close()\n",
    "# # print(lastest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
